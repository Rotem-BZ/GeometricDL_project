@article{cooperation_and_codenames, title={Cooperation and Codenames: Understanding Natural Language Processing via Codenames}, volume={15}, url={https://ojs.aaai.org/index.php/AIIDE/article/view/5239}, abstractNote={&lt;p&gt;Codenames – a board game by Vlaada Chvátil – is a game that requires deep, multi-modal language understanding. One player, the codemaster, gives a clue to another set of players, the guessers, and the guessers must determine which of 25 possible words on the board correspond to the clue. The nature of the game requires understanding language in a multi-modal manner – e.g., the clue ‘cold’ could refer to temperature or disease. The recently proposed Codenames AI Competition seeks to advance natural language processing, by using Codenames as a testbed for multi-modal language understanding. In this work, we evaluate a number of different natural language processing techniques (ranging from neural approaches to classical knowledge-base methods) in the context of the Codenames AI framework, attempting to determine how different approaches perform. The agents are evaluated when working with identical agents, as well as evaluated with all other approaches – i.e., when they have no knowledge about their partner.&lt;/p&gt;}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment}, author={Kim, Andrew and Ruzmaykin, Maxim and Truong, Aaron and Summerville, Adam}, year={2019}, month={Oct.}, pages={160-166} }

@article{Divya,
	title={Playing codenames with language graphs and word embeddings},
	year="2021",
	author="Koyyalagunta, Divya and Sun, Anna and Draelos, Rachel Lea and Rudin, Cynthia",
	journal={Journal of Artificial Intelligence Research},
	volume={71},
	pages={319--346},
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@article{hanabi_challenge,
	title = {The Hanabi challenge: A new frontier for AI research},
	journal = {Artificial Intelligence},
	volume = {280},
	pages = {103216},
	year = {2020},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/j.artint.2019.103216},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370219300116},
	author = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
	keywords = {Multi-agent learning, Challenge paper, Reinforcement learning, Games, Theory of mind, Communication, Imperfect information, Cooperative},
	abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.}
}

@misc{passive-RL,
	doi = {10.48550/ARXIV.2110.14020},	
	url = {https://arxiv.org/abs/2110.14020},	
	author = {Ostrovski, Georg and Castro, Pablo Samuel and Dabney, Will},	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {The Difficulty of Passive Learning in Deep Reinforcement Learning},	
	publisher = {arXiv},	
	year = {2021},	
	copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{CRM,
	title={Counterfactual risk minimization: Learning from logged bandit feedback},
	author={Swaminathan, Adith and Joachims, Thorsten},
	booktitle={International Conference on Machine Learning},
	pages={814--823},
	year={2015},
	organization={PMLR}
}

@online{url,
	title = {Code (GitHub)},
	url = {https://github.com/Rotem-BZ/EGG}}

@online{Prolific,
	title = {Prolific},
	url = {https://www.prolific.co/}}

@online{wikipedia,
	title = {Wikipedia - codenames},
	url = {https://en.wikipedia.org/wiki/Codenames_(board_game)}}

@online{towardsdatascience,
	title = {Do we need deep graph neural networks?},
	url = {https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59}}

@article{oono2019graph,
	title={Graph neural networks exponentially lose expressive power for node classification},
	author={Oono, Kenta and Suzuki, Taiji},
	journal={arXiv preprint arXiv:1905.10947},
	year={2019}
}

@article{alon2020bottleneck,
	title={On the bottleneck of graph neural networks and its practical implications},
	author={Alon, Uri and Yahav, Eran},
	journal={arXiv preprint arXiv:2006.05205},
	year={2020}
}

@article{rong2019dropedge,
	title={Dropedge: Towards deep graph convolutional networks on node classification},
	author={Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
	journal={arXiv preprint arXiv:1907.10903},
	year={2019}
}

@inproceedings{li2018deeper,
	title={Deeper insights into graph convolutional networks for semi-supervised learning},
	author={Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
	booktitle={Thirty-Second AAAI conference on artificial intelligence},
	year={2018}
}

@inproceedings{li2019deepgcns,
	title={Deepgcns: Can gcns go as deep as cnns?},
	author={Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={9267--9276},
	year={2019}
}

@article{zeng2021decoupling,
	title={Decoupling the depth and scope of graph neural networks},
	author={Zeng, Hanqing and Zhang, Muhan and Xia, Yinglong and Srivastava, Ajitesh and Malevich, Andrey and Kannan, Rajgopal and Prasanna, Viktor and Jin, Long and Chen, Ren},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={19665--19679},
	year={2021}
}

@inproceedings{gori2005new,
	title={A new model for learning in graph domains},
	author={Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
	booktitle={Proceedings. 2005 IEEE international joint conference on neural networks},
	volume={2},
	number={2005},
	pages={729--734},
	year={2005}
}

@article{scarselli2008graph,
	title={The graph neural network model},
	author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	journal={IEEE transactions on neural networks},
	volume={20},
	number={1},
	pages={61--80},
	year={2008},
	publisher={IEEE}
}

@article{hu2020open,
	title={Open graph benchmark: Datasets for machine learning on graphs},
	author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={22118--22133},
	year={2020}
}